{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10617,
     "status": "ok",
     "timestamp": 1729307085000,
     "user": {
      "displayName": "Nam Le",
      "userId": "10910051602335296858"
     },
     "user_tz": -420
    },
    "id": "k4UhRVN-PNYi",
    "outputId": "3a910cb5-bb67-4fa2-d833-c4790dab293c"
   },
   "outputs": [],
   "source": [
    "# Install some dependencies if it is necessary\n",
    "# !pip install torch_geometric rdkit torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "YKUU9J_HnRwg"
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "import time\n",
    "import argparse\n",
    "import sys\n",
    "import torch\n",
    "from torch import optim\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn import metrics\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.nn.modules.container import ModuleList\n",
    "from torch_geometric.nn import (\n",
    "    GATConv,\n",
    "    SAGPooling,\n",
    "    LayerNorm,\n",
    "    global_mean_pool,\n",
    "    max_pool_neighbor_x,\n",
    "    global_add_pool,\n",
    ")\n",
    "\n",
    "import itertools\n",
    "from collections import defaultdict\n",
    "from operator import neg\n",
    "import random\n",
    "import math\n",
    "\n",
    "import csv\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch_geometric.data import Data, Batch\n",
    "from rdkit import Chem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Common configuration: model going to be used for the prediction or to be saved in the training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory configuration\n",
    "data_dir = \"data\"\n",
    "model_dir = \"models\"\n",
    "model_name = \"case28\"\n",
    "\n",
    "# Change mode to \"prediction\" to ignore training and perform prediction on test set.\n",
    "mode = \"train\"\n",
    "\n",
    "\n",
    "model_acc_file = f\"{model_dir}/acc/{model_name}.pth\"\n",
    "model_roc_file = f\"{model_dir}/roc/{model_name}.pth\"\n",
    "model_prc_file = f\"{model_dir}/prc/{model_name}.pth\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tunning parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modified the tunning parameters below to train on different cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### Tunning parameters #######\n",
    "\n",
    "# Number of epochs\n",
    "n_epochs = 300\n",
    "\n",
    "# SagPooling ratio & min score. \n",
    "# Set sp_ratio to None to disable ratio in SagPooling\n",
    "sp_ratio = None\n",
    "sp_min_score = None\n",
    "\n",
    "# Enable using gpu\n",
    "use_cuda = True\n",
    "\n",
    "# Use activation function for CoAttention Layer\n",
    "use_activation_fn = False\n",
    "\n",
    "# Use ComplEx instead of RESCAL\n",
    "use_ComplEx = True\n",
    "\n",
    "# Use improved CoAttention Layer\n",
    "# Could be \"original\" || \"improved\" || \"multihead\"\n",
    "co_attention_method = \"multihead\"\n",
    "\n",
    "# Use Explicit Valence\n",
    "# refers to the number of chemical bonds explicitly represented for an atom \n",
    "# within a molecule in a chemical structure.\n",
    "use_explicit_valence = False\n",
    "\n",
    "# Number of GAT layers\n",
    "num_GAT_layers = 4\n",
    "\n",
    "# Number of GAT multiheads\n",
    "num_GAT_multiheads = 2\n",
    "\n",
    "#################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14083,
     "status": "ok",
     "timestamp": 1729307119217,
     "user": {
      "displayName": "Nam Le",
      "userId": "10910051602335296858"
     },
     "user_tz": -420
    },
    "id": "Ws9OIlmzPrsJ",
    "outputId": "f4ec9a42-5720-436e-fa2a-544d564381c4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/d/Documents/Github/glsofort/SSI-DDI-test/notebook/data_preprocessing.py:113: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3683.)\n",
      "  return undirected_edge_list.T, features\n"
     ]
    }
   ],
   "source": [
    "# Data directory\n",
    "data_dir = \"data\"\n",
    "\n",
    "df_drugs_smiles = pd.read_csv(f'{data_dir}/drug_smiles.csv')\n",
    "\n",
    "DRUG_TO_INDX_DICT = {drug_id: indx for indx, drug_id in enumerate(df_drugs_smiles['drug_id'])}\n",
    "\n",
    "# drug_id_mol_graph_tup (list of tuple): Contains drug information of all drugs as a list of tuples.\n",
    "# each tuple has the following elements:\n",
    "# - id: drug ID provided in drug_smiles.csv\n",
    "# - mol_graph: rdkit Mol object extracted from Smiles string\n",
    "# note: smiles object only represent the connectivity of atoms in a molecule in a text form.\n",
    "#       where Rdkit Mol object capture a complete graph-based representation of molecule.\n",
    "#       making it easier for feature extraction (atomic features, bond features, ...)\n",
    "drug_id_mol_graph_tup = [(id, Chem.MolFromSmiles(smiles.strip())) for id, smiles in zip(df_drugs_smiles['drug_id'], df_drugs_smiles['smiles'])]\n",
    "\n",
    "# ATOM_MAX_NUM (int): Max number of atoms in every drugs in drug_id_mol_graph_tup list.\n",
    "ATOM_MAX_NUM = np.max([m[1].GetNumAtoms() for m in drug_id_mol_graph_tup])\n",
    "\n",
    "# AVAILABLE_ATOM_SYMBOLS (list of str): Contains all symbols of all atoms of each drugs in drug_id_mol_graph_tup list.\n",
    "# atoms have symbols like Ag, Na, Br, ...\n",
    "AVAILABLE_ATOM_SYMBOLS = list({a.GetSymbol() for a in itertools.chain.from_iterable(m[1].GetAtoms() for m in drug_id_mol_graph_tup)})\n",
    "\n",
    "# AVAILABLE_ATOM_DEGREES (list of int): Contains all degree of all atoms of all drugs in drug_id_mol_graph_tup list\n",
    "# an atom degree is the total number of connections between atoms (Simply it's degree of nodes).\n",
    "AVAILABLE_ATOM_DEGREES = list({a.GetDegree() for a in itertools.chain.from_iterable(m[1].GetAtoms() for m in drug_id_mol_graph_tup)})\n",
    "\n",
    "# AVAILABLE_ATOM_TOTAL_HS (list of int): Contains all Hydrogens attached to the atoms of all drugs in drug_id_mol_graph_tup list\n",
    "AVAILABLE_ATOM_TOTAL_HS = list({a.GetTotalNumHs() for a in itertools.chain.from_iterable(m[1].GetAtoms() for m in drug_id_mol_graph_tup)})\n",
    "\n",
    "# max_valence (int): A maximum number of Implicit Valence in all atoms of all drugs. Minimum = 9.\n",
    "# implicit valance: describes the \"wants\" of complete bonds. For eg. C=0 wants to complete 4 bonds,\n",
    "#                   because C can form 4 bonds with other atoms or molecule\n",
    "#                   Tthen Implicit Valance = 2.\n",
    "max_valence = max(a.GetImplicitValence() for a in itertools.chain.from_iterable(m[1].GetAtoms() for m in drug_id_mol_graph_tup))\n",
    "max_valence = max(max_valence, 9)\n",
    "\n",
    "# AVAILABLE_ATOM_VALENCE (NumPy array of int): generates a NumPy array of integers, starting from 0 and ending at value. \n",
    "# the result is an array containing all integers from 0 up to max_valence (but not including max_valence).\n",
    "AVAILABLE_ATOM_VALENCE = np.arange(max_valence + 1)\n",
    "\n",
    "# MAX_ATOM_FC (int): maximum absolute formal charge of all atoms in a list of molecular structures.\n",
    "# formal Charge: Provide a way to estimate ad track electron distribution within a molecule.\n",
    "MAX_ATOM_FC = abs(np.max([a.GetFormalCharge() for a in itertools.chain.from_iterable(m[1].GetAtoms() for m in drug_id_mol_graph_tup)]))\n",
    "MAX_ATOM_FC = MAX_ATOM_FC if MAX_ATOM_FC else 0\n",
    "\n",
    "# MAX_RADICAL_ELC (int): calculates the maximum absolute number of radical electrons found across all atoms in a list of molecular structures.\n",
    "# radical electrons: this is important feature, play importan participation in reactions. \n",
    "#                    this is the key in many chemical.\n",
    "MAX_RADICAL_ELC = abs(np.max([a.GetNumRadicalElectrons() for a in itertools.chain.from_iterable(m[1].GetAtoms() for m in drug_id_mol_graph_tup)]))\n",
    "MAX_RADICAL_ELC = MAX_RADICAL_ELC if MAX_RADICAL_ELC else 0\n",
    "\n",
    "\n",
    "def one_of_k_encoding_unk(x, allowable_set):\n",
    "    \"\"\"\n",
    "    Get all matched elements in allowable set.\n",
    "\n",
    "    Args:\n",
    "        x (any): element to check.\n",
    "        allowable_set (list of any): List of any elements are allowed.\n",
    "\n",
    "    Returns:\n",
    "        list of any: Return list of any elements in allowable_set equal to x. \n",
    "            If x not in allowable_set then x = last element of the allowable set.\n",
    "    \"\"\"\n",
    "    if x not in allowable_set:\n",
    "        # This would be Unknown\n",
    "        x = allowable_set[-1]\n",
    "    return list(map(lambda s: x == s, allowable_set))\n",
    "\n",
    "def atom_features(atom,\n",
    "                explicit_H=True,\n",
    "                use_chirality=False,\n",
    "                use_explicit_valence=False):\n",
    "    \"\"\"\n",
    "    Get all features of an atom.\n",
    "\n",
    "    Args:\n",
    "        atom (Atom): atom to be performed features extractation.\n",
    "        explicit_H (boolean): include Explicit Hydrogens. Default = True.\n",
    "        use_chirality (boolean): include Chirality feature. Default = False.\n",
    "        use_explicit_valence (boolean): include Explicit Valence feature. Default = True.\n",
    "\n",
    "    Returns:\n",
    "        NumPy array: Represents an one hot encoding feature vector of an atom.\n",
    "    \"\"\"\n",
    "\n",
    "    results = one_of_k_encoding_unk(\n",
    "        atom.GetSymbol(),\n",
    "        ['C','N','O', 'S','F','Si','P', 'Cl','Br','Mg','Na','Ca','Fe','As','Al','I','B','V','K','Tl',\n",
    "            'Yb','Sb','Sn','Ag','Pd','Co','Se','Ti','Zn','H', 'Li','Ge','Cu','Au','Ni','Cd','In',\n",
    "            'Mn','Zr','Cr','Pt','Hg','Pb','Unknown'\n",
    "        ]) + [atom.GetDegree()/10, atom.GetImplicitValence(), \n",
    "                atom.GetFormalCharge(), atom.GetNumRadicalElectrons()] + \\\n",
    "                one_of_k_encoding_unk(atom.GetHybridization(), [\n",
    "                Chem.rdchem.HybridizationType.SP, Chem.rdchem.HybridizationType.SP2,\n",
    "                Chem.rdchem.HybridizationType.SP3, Chem.rdchem.HybridizationType.\n",
    "                                    SP3D, Chem.rdchem.HybridizationType.SP3D2\n",
    "                ]) + [atom.GetIsAromatic()]\n",
    "    # in case of explicit hydrogen(QM8, QM9), avoid calling `GetTotalNumHs`\n",
    "    if explicit_H:\n",
    "        results = results + [atom.GetTotalNumHs()]\n",
    "\n",
    "    # chilarity: chilarity molecule can have very different properties and biological effects depending on\n",
    "    # their orientation. This influence how the molecule interact with biological systems or other chemical entities.\n",
    "    if use_chirality:\n",
    "        try:\n",
    "            results = results + one_of_k_encoding_unk(\n",
    "            atom.GetProp('_CIPCode'),\n",
    "            ['R', 'S']) + [atom.HasProp('_ChiralityPossible')]\n",
    "        except:\n",
    "            results = results + [False, False\n",
    "                            ] + [atom.HasProp('_ChiralityPossible')]\n",
    "\n",
    "    if use_explicit_valence:\n",
    "        results = results + [atom.GetExplicitValence()]\n",
    "\n",
    "    results = np.array(results).astype(np.float32)\n",
    "\n",
    "    return torch.from_numpy(results)\n",
    "\n",
    "def get_mol_edge_list_and_feat_mtx(mol_graph):\n",
    "    \"\"\"\n",
    "    Get molecule edge list and features matrix\n",
    "\n",
    "    Args:\n",
    "        mol_graph (Mol): rdkit mol object of a drug.\n",
    "\n",
    "    Returns:\n",
    "        tuple: edge list and features matrix\n",
    "    \"\"\"\n",
    "    # features (list of tuple): Contains drug id and features of all atom as a list of tuples.\n",
    "    # each tuple has the following elements:\n",
    "    # - index: index of an atom.\n",
    "    # - features (Numpy array): features of an atom\n",
    "    features = [(atom.GetIdx(), atom_features(atom)) for atom in mol_graph.GetAtoms()]\n",
    "\n",
    "    # to make sure that the feature matrix is aligned according to the idx of the atom\n",
    "    features.sort()\n",
    "    _, features = zip(*features)\n",
    "    features = torch.stack(features)\n",
    "\n",
    "    # GetBonds(): retrive all bonds in the Mol object. Each bond represent connections between 2 atoms.\n",
    "    # for each bond, retrieve the indices of 2 connected by a bond to form a tuple (start_atom_index, end_atom_index)\n",
    "    # representing edge between 2 atoms.\n",
    "    # convert to a Torch Tensor (2D tensor) of a matrix.\n",
    "    # each row represent an edge, the first column and second column being the indices of the bonded atoms.\n",
    "    edge_list = torch.LongTensor([(b.GetBeginAtomIdx(), b.GetEndAtomIdx()) for b in mol_graph.GetBonds()])\n",
    "    undirected_edge_list = torch.cat([edge_list, edge_list[:, [1, 0]]], dim=0) if len(edge_list) else edge_list\n",
    "    \n",
    "    return undirected_edge_list.T, features\n",
    "\n",
    "\n",
    "# MOL_EDGE_LIST_FEAT_MTX (dict): each field in the dict is (drug_id: (edge_list, feature_matrix))\n",
    "# edge_list (shape: (num_edges, 2)): in the mol object, retrive all bonds. \"bonds\" means the connection between atoms.\n",
    "#           list of edges tuple(atom1 index, atom2 index).\n",
    "# undirected_edge_list (shape: (2, 2 * num_edges)): since molecular bonds are typically undirected, we make each directed edge bidirectional.\n",
    "#                      first row ontains the start nodes of each edge, second row contains the end nodes of each edge.\n",
    "# undirected_edge_list.T (shape (2*num_edges, 2)): 2 columns, first col is start node(atom1) and second one is end node(atom2).\n",
    "\n",
    "MOL_EDGE_LIST_FEAT_MTX = {drug_id: get_mol_edge_list_and_feat_mtx(mol) \n",
    "                                for drug_id, mol in drug_id_mol_graph_tup}\n",
    "MOL_EDGE_LIST_FEAT_MTX = {drug_id: mol for drug_id, mol in MOL_EDGE_LIST_FEAT_MTX.items() if mol is not None}\n",
    "\n",
    "# TOTAL_ATOM_FEATS (int): total features of an atom.\n",
    "TOTAL_ATOM_FEATS = (next(iter(MOL_EDGE_LIST_FEAT_MTX.values()))[1].shape[-1])\n",
    "\n",
    "\n",
    "##### DDI statistics and counting #######\n",
    "df_all_pos_ddi = pd.read_csv(f'{data_dir}/ddis.csv')\n",
    "all_pos_tup = [(h, t, r) for h, t, r in zip(df_all_pos_ddi['d1'], df_all_pos_ddi['d2'], df_all_pos_ddi['type'])]\n",
    "\n",
    "# ALL_DRUG_IDS (Numpy array of int): list of all drug ids\n",
    "ALL_DRUG_IDS, _ = zip(*drug_id_mol_graph_tup)\n",
    "ALL_DRUG_IDS = np.array(list(set(ALL_DRUG_IDS)))\n",
    "\n",
    "# ALL_TRUE_H_WITH_TR (dict): all drug 1 with drug 2 and their relationship\n",
    "ALL_TRUE_H_WITH_TR = defaultdict(list)\n",
    "\n",
    "# ALL_TRUE_T_WITH_HR (dict): all drug 2 with drug 1 and their relationship\n",
    "ALL_TRUE_T_WITH_HR = defaultdict(list)\n",
    "\n",
    "# FREQ_REL (dict): calculate number of relationships appeared.\n",
    "FREQ_REL = defaultdict(int)\n",
    "\n",
    "# ALL_H_WITH_R (dict): mark the dict relationships with head drug.\n",
    "ALL_H_WITH_R = defaultdict(dict)\n",
    "\n",
    "# ALL_T_WITH_R (dict): mark the dict relationships with tail drug.\n",
    "ALL_T_WITH_R = defaultdict(dict)\n",
    "\n",
    "# ALL_TAIL_PER_HEAD (dict): freaquent of relationships / (length of list drug tail in relationships \"r\")\n",
    "ALL_TAIL_PER_HEAD = {}\n",
    "\n",
    "# ALL_HEAD_PER_TAIL (dict): freaquent of relationships / (length of list drug head in relationships \"r\")\n",
    "ALL_HEAD_PER_TAIL = {}\n",
    "\n",
    "for h, t, r in all_pos_tup:\n",
    "    ALL_TRUE_H_WITH_TR[(t, r)].append(h)\n",
    "    ALL_TRUE_T_WITH_HR[(h, r)].append(t)\n",
    "    FREQ_REL[r] += 1.0\n",
    "    ALL_H_WITH_R[r][h] = 1\n",
    "    ALL_T_WITH_R[r][t] = 1\n",
    "\n",
    "for t, r in ALL_TRUE_H_WITH_TR:\n",
    "    ALL_TRUE_H_WITH_TR[(t, r)] = np.array(list(set(ALL_TRUE_H_WITH_TR[(t, r)])))\n",
    "for h, r in ALL_TRUE_T_WITH_HR:\n",
    "    ALL_TRUE_T_WITH_HR[(h, r)] = np.array(list(set(ALL_TRUE_T_WITH_HR[(h, r)])))\n",
    "\n",
    "for r in FREQ_REL:\n",
    "    ALL_H_WITH_R[r] = np.array(list(ALL_H_WITH_R[r].keys()))\n",
    "    ALL_T_WITH_R[r] = np.array(list(ALL_T_WITH_R[r].keys()))\n",
    "    ALL_HEAD_PER_TAIL[r] = FREQ_REL[r] / len(ALL_T_WITH_R[r])\n",
    "    ALL_TAIL_PER_HEAD[r] = FREQ_REL[r] / len(ALL_H_WITH_R[r])\n",
    "\n",
    "#######    ****** ###############\n",
    "\n",
    "class DrugDataset(Dataset):\n",
    "    def __init__(self, tri_list, ratio=1.0,  neg_ent=1, disjoint_split=True, shuffle=True):\n",
    "        ''''disjoint_split: Consider whether entities should appear in one and only one split of the dataset\n",
    "        ''' \n",
    "        self.neg_ent = neg_ent\n",
    "        self.tri_list = []\n",
    "        self.ratio = ratio\n",
    "\n",
    "        for h, t, r, *_ in tri_list:\n",
    "            if ((h in MOL_EDGE_LIST_FEAT_MTX) and (t in MOL_EDGE_LIST_FEAT_MTX)):\n",
    "                self.tri_list.append((h, t, r))\n",
    "\n",
    "        if disjoint_split:\n",
    "            d1, d2, *_ = zip(*self.tri_list)\n",
    "            self.drug_ids = np.array(list(set(d1 + d2)))\n",
    "        else:\n",
    "            self.drug_ids = ALL_DRUG_IDS\n",
    "\n",
    "        self.drug_ids = np.array([id for id in self.drug_ids if id in MOL_EDGE_LIST_FEAT_MTX])\n",
    "        \n",
    "        if shuffle:\n",
    "            random.shuffle(self.tri_list)\n",
    "        limit = math.ceil(len(self.tri_list) * ratio)\n",
    "        self.tri_list = self.tri_list[:limit]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tri_list)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.tri_list[index]\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        \"\"\"\n",
    "        This function is for createing positive and negative sampling.\n",
    "        \"\"\"\n",
    "        pos_rels = []\n",
    "        pos_h_samples = []\n",
    "        pos_t_samples = []\n",
    "        neg_rels = []\n",
    "        neg_h_samples = []\n",
    "        neg_t_samples = []\n",
    "\n",
    "        for h, t, r in batch:\n",
    "            pos_rels.append(r)\n",
    "            h_data = self.__create_graph_data(h)\n",
    "            t_data = self.__create_graph_data(t)\n",
    "            pos_h_samples.append(h_data)\n",
    "            pos_t_samples.append(t_data)\n",
    "\n",
    "            neg_heads, neg_tails = self.__normal_batch(h, t, r, self.neg_ent)\n",
    "\n",
    "            for neg_h in neg_heads:\n",
    "                neg_rels.append(r)\n",
    "                neg_h_samples.append(self.__create_graph_data(neg_h))\n",
    "                neg_t_samples.append(t_data)\n",
    "\n",
    "            for neg_t in neg_tails:\n",
    "                neg_rels.append(r)\n",
    "                neg_h_samples.append(h_data)\n",
    "                neg_t_samples.append(self.__create_graph_data(neg_t))\n",
    "\n",
    "        pos_h_samples = Batch.from_data_list(pos_h_samples)\n",
    "        pos_t_samples = Batch.from_data_list(pos_t_samples)\n",
    "        pos_rels = torch.LongTensor(pos_rels)\n",
    "        pos_tri = (pos_h_samples, pos_t_samples, pos_rels)\n",
    "\n",
    "        neg_h_samples = Batch.from_data_list(neg_h_samples)\n",
    "        neg_t_samples = Batch.from_data_list(neg_t_samples)\n",
    "        neg_rels = torch.LongTensor(neg_rels)\n",
    "        neg_tri = (neg_h_samples, neg_t_samples, neg_rels)\n",
    "\n",
    "        return pos_tri, neg_tri\n",
    "            \n",
    "    def __create_graph_data(self, id):\n",
    "        edge_index = MOL_EDGE_LIST_FEAT_MTX[id][0]\n",
    "        features = MOL_EDGE_LIST_FEAT_MTX[id][1]\n",
    "\n",
    "        return Data(x=features, edge_index=edge_index)\n",
    "\n",
    "    def __corrupt_ent(self, other_ent, r, other_ent_with_r_dict, max_num=1):\n",
    "        corrupted_ents = []\n",
    "        current_size = 0\n",
    "        while current_size < max_num:\n",
    "            candidates = np.random.choice(self.drug_ids, (max_num - current_size) * 2)\n",
    "            mask = np.isin(candidates, other_ent_with_r_dict[(other_ent, r)], assume_unique=True, invert=True)\n",
    "            corrupted_ents.append(candidates[mask])\n",
    "            current_size += len(corrupted_ents[-1])\n",
    "        \n",
    "        if corrupted_ents != []:\n",
    "            corrupted_ents = np.concatenate(corrupted_ents)\n",
    "\n",
    "        return np.asarray(corrupted_ents[:max_num])\n",
    "        \n",
    "    def __corrupt_head(self, t, r, n=1):\n",
    "        return self.__corrupt_ent(t, r, ALL_TRUE_H_WITH_TR, n)\n",
    "\n",
    "    def __corrupt_tail(self, h, r, n=1):\n",
    "        return self.__corrupt_ent(h, r, ALL_TRUE_T_WITH_HR, n)\n",
    "    \n",
    "    def __normal_batch(self, h, t, r, neg_size):\n",
    "        neg_size_h = 0\n",
    "        neg_size_t = 0\n",
    "        prob = ALL_TAIL_PER_HEAD[r] / (ALL_TAIL_PER_HEAD[r] + ALL_HEAD_PER_TAIL[r])\n",
    "        for i in range(neg_size):\n",
    "            if random.random() < prob:\n",
    "                neg_size_h += 1\n",
    "            else:\n",
    "                neg_size_t +=1\n",
    "        \n",
    "        return (self.__corrupt_head(t, r, neg_size_h),\n",
    "                self.__corrupt_tail(h, r, neg_size_t))  \n",
    "\n",
    "\n",
    "class DrugDataLoader(DataLoader):\n",
    "    # Each epoch will generate random batches of data with the specified batch_size\n",
    "    def __init__(self, data, **kwargs):\n",
    "        super().__init__(data, collate_fn=data.collate_fn, **kwargs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "Epochs: 300\n",
      "Total of atom features: 55\n"
     ]
    }
   ],
   "source": [
    "# Total atom features\n",
    "n_atom_feats = TOTAL_ATOM_FEATS\n",
    "# Not use\n",
    "n_atom_hid = 64\n",
    "# Total interactions information in the Interaction_information.csv\n",
    "rel_total = 86\n",
    "lr = 1e-2\n",
    "weight_decay = 5e-4\n",
    "neg_samples = 1\n",
    "# Represents the number of samples (or graph instances) loaded in each batch during the training process.\n",
    "batch_size = 1024\n",
    "data_size_ratio = 1\n",
    "\n",
    "# Knowledge graph embeding dimension\n",
    "kge_dim = 64\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() and use_cuda else \"cpu\"\n",
    "\n",
    "print(device)\n",
    "print(f\"Epochs: {n_epochs}\")\n",
    "print(f\"Total of atom features: {TOTAL_ATOM_FEATS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ddi_train = pd.read_csv(f\"{data_dir}/ddi_training.csv\")\n",
    "df_ddi_val = pd.read_csv(f\"{data_dir}/ddi_validation.csv\")\n",
    "df_ddi_test = pd.read_csv(f\"{data_dir}/ddi_test.csv\")\n",
    "\n",
    "\n",
    "train_tup = [\n",
    "    (h, t, r)\n",
    "    for h, t, r in zip(df_ddi_train[\"d1\"], df_ddi_train[\"d2\"], df_ddi_train[\"type\"])\n",
    "]\n",
    "val_tup = [\n",
    "    (h, t, r) for h, t, r in zip(df_ddi_val[\"d1\"], df_ddi_val[\"d2\"], df_ddi_val[\"type\"])\n",
    "]\n",
    "test_tup = [\n",
    "    (h, t, r)\n",
    "    for h, t, r in zip(df_ddi_test[\"d1\"], df_ddi_test[\"d2\"], df_ddi_test[\"type\"])\n",
    "]\n",
    "\n",
    "train_data = DrugDataset(train_tup, ratio=data_size_ratio, neg_ent=neg_samples)\n",
    "val_data = DrugDataset(val_tup, ratio=data_size_ratio, disjoint_split=False)\n",
    "test_data = DrugDataset(test_tup, disjoint_split=False)\n",
    "\n",
    "print(\n",
    "    f\"Training with {len(train_data)} samples, validating with {len(val_data)}, and testing with {len(test_data)}\"\n",
    ")\n",
    "\n",
    "train_data_loader = DrugDataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "val_data_loader = DrugDataLoader(val_data, batch_size=batch_size * 3)\n",
    "test_data_loader = DrugDataLoader(test_data, batch_size=batch_size * 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_tunning_parameters():\n",
    "    \"\"\"\n",
    "    Print tunning parameters\n",
    "    \"\"\"\n",
    "    print()\n",
    "    print(\"####### Tunning parameters #######\")\n",
    "    print()\n",
    "    \n",
    "    print(\"n_epochs =\", n_epochs)\n",
    "    print(\"use_cuda =\", use_cuda)\n",
    "    print()\n",
    "    print(\"num_GAT_layers = \", num_GAT_layers)\n",
    "    print(\"num_GAT_multiheads = \", num_GAT_multiheads)\n",
    "    print()\n",
    "    print(\"sp_ratio =\", sp_ratio)\n",
    "    print(\"sp_min_score =\", sp_min_score)\n",
    "    print()\n",
    "    print(\"use_explicit_valence =\", use_explicit_valence)\n",
    "    print()\n",
    "    print(\"use_activation_fn =\", use_activation_fn)\n",
    "    print()\n",
    "    print(\"use_ComplEx =\", use_ComplEx)\n",
    "    print()\n",
    "    print(\"co_attention_method =\", co_attention_method)\n",
    "    \n",
    "    print()\n",
    "    print(\"#################################\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section contains custom layers can be used in model. `CoAttentionLayer` and `RESCAL` are from the original paper. Our improvment layers are `MultiheadCoAttentionLayer` and `ComplEx`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "qfaqIm8ZOwx9"
   },
   "outputs": [],
   "source": [
    "class CoAttentionLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, n_features, use_activation_fn=True):\n",
    "        super().__init__()\n",
    "        self.n_features = n_features\n",
    "        self.w_q = nn.Parameter(torch.zeros(n_features, n_features // 2))\n",
    "        self.w_k = nn.Parameter(torch.zeros(n_features, n_features // 2))\n",
    "        self.bias = nn.Parameter(torch.zeros(n_features // 2))\n",
    "        self.a = nn.Parameter(torch.zeros(n_features // 2))\n",
    "        self.use_activation_fn = use_activation_fn\n",
    "\n",
    "        nn.init.xavier_uniform_(self.w_q)\n",
    "        nn.init.xavier_uniform_(self.w_k)\n",
    "        nn.init.xavier_uniform_(self.bias.view(*self.bias.shape, -1))\n",
    "        nn.init.xavier_uniform_(self.a.view(*self.a.shape, -1))\n",
    "\n",
    "    def forward(self, receiver, attendant):\n",
    "        # receiver.shape  = (1024, 4, 64)\n",
    "        # attendant.shape = (1024, 4, 64)\n",
    "        keys = receiver @ self.w_k\n",
    "        queries = attendant @ self.w_q\n",
    "        # values = receiver @ self.w_v\n",
    "        values = receiver\n",
    "\n",
    "        # queries.shape = (1024, 4, 32)\n",
    "        # keys.shape = (1024, 4, 32)\n",
    "        e_activations = queries.unsqueeze(-3) + keys.unsqueeze(-2) + self.bias\n",
    "        if self.use_activation_fn:\n",
    "            e_scores = torch.tanh(e_activations) @ self.a\n",
    "        else:\n",
    "            e_scores = e_activations @ self.a\n",
    "        attentions = e_scores\n",
    "\n",
    "        return attentions\n",
    "\n",
    "\n",
    "class MultiheadCoAttentionLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Using multi-head co-attention\n",
    "    \"\"\"\n",
    "    def __init__(self, n_features, use_activation_fn=True, dropout=0.1, n_heads=2):\n",
    "        super().__init__()\n",
    "        self.n_features = n_features\n",
    "        self.n_heads = n_heads\n",
    "\n",
    "        \n",
    "        self.W_q = nn.ParameterList([torch.zeros(self.n_features, self.n_features // n_heads) for _ in range(n_heads)])\n",
    "        self.W_k = nn.ParameterList([torch.zeros(self.n_features, self.n_features // n_heads) for _ in range(n_heads)])\n",
    "        \n",
    "        self.a = nn.Parameter(torch.zeros(self.n_features))\n",
    "        self.bias = nn.ParameterList([torch.zeros(self.n_features // n_heads) for _ in range(n_heads)])\n",
    "        \n",
    "        self.use_activation_fn = use_activation_fn\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        for i in range(n_heads):\n",
    "            nn.init.xavier_uniform_(self.W_q[i])\n",
    "            nn.init.xavier_uniform_(self.W_k[i])\n",
    "            nn.init.xavier_uniform_(self.bias[i].view(*self.bias[i].shape, -1))\n",
    "        \n",
    "        nn.init.xavier_uniform_(self.a.view(*self.a.shape, -1))\n",
    "\n",
    "    def forward(self, receiver, attendant):\n",
    "        # receiver.shape  = (1024, 4, 64)\n",
    "        # attendant.shape = (1024, 4, 64)\n",
    "\n",
    "        # Compute attention score for each head\n",
    "        head_outputs = []\n",
    "        for i in range(self.n_heads):\n",
    "            keys = receiver @ self.W_k[i]\n",
    "            queries = attendant @ self.W_q[i]\n",
    "            # print(\"keys.shape = \", keys.shape)\n",
    "            e_activations = queries.unsqueeze(-3) + keys.unsqueeze(-2) + self.bias[i]\n",
    "            # print(\"e_activations.shape = \", e_activations.shape)\n",
    "            head_outputs.append(e_activations)\n",
    "\n",
    "        # Average the outputs from all heads\n",
    "        # e_activations_concat.shape = (1024, 4, 4, 32)\n",
    "        e_activations_concat = torch.cat(head_outputs, dim=-1)\n",
    "        # print(\"e_activations_avg.shape = \", e_activations_avg.shape)\n",
    "        \n",
    "        if self.use_activation_fn:\n",
    "            e_scores = torch.tanh(e_activations_concat) @ self.a\n",
    "        else:\n",
    "            e_scores = e_activations_concat @ self.a\n",
    "\n",
    "        # attentions.shape = (1024, 4, 4)\n",
    "        attentions = e_scores\n",
    "\n",
    "        return attentions\n",
    "\n",
    "class RESCAL(nn.Module):\n",
    "    \"\"\"\n",
    "    RESCAL is like the Dismults but for matrices.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_rels, n_features):\n",
    "        \"\"\"\n",
    "        n_rels: number of relations = 86\n",
    "        n_features: kge_dim = 64\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.n_rels = n_rels\n",
    "        self.n_features = n_features\n",
    "        # Embedding layer\n",
    "        self.rel_emb = nn.Embedding(self.n_rels, n_features * n_features)\n",
    "        #  Initializes the embedding weights with the Xavier uniform distribution, which helps maintain the scale of gradients during training\n",
    "        nn.init.xavier_uniform_(self.rel_emb.weight)\n",
    "\n",
    "    def forward(self, heads, tails, rels, alpha_scores):\n",
    "        rels = self.rel_emb(rels)\n",
    "        rels = F.normalize(rels, dim=-1)\n",
    "        heads = F.normalize(heads, dim=-1)\n",
    "        tails = F.normalize(tails, dim=-1)\n",
    "        # print(rels.shape)\n",
    "        # Convert shape (1024, 4096) to (1024, 64, 64) for dot product\n",
    "        rels = rels.view(-1, self.n_features, self.n_features)\n",
    "        # print(rels.shape)\n",
    "        # (1024, 4, 64) @ (1024, 64, 64) = (1024, 4, 64) @ (1024, 64, 4) = (1024, 4, 4)\n",
    "        scores = heads @ rels @ tails.transpose(-2, -1)\n",
    "\n",
    "        # alpha_scores.shape = (1024, 4, 4)\n",
    "        # scores.shape = (1024, 4, 4)\n",
    "        if alpha_scores is not None:\n",
    "            scores = alpha_scores * scores\n",
    "        # print(scores.shape)\n",
    "        \n",
    "        # sum the last 2 dimensions\n",
    "        scores = scores.sum(dim=(-2, -1))\n",
    "        \n",
    "        # print(scores.shape)\n",
    "        # Shape(1024,)\n",
    "        return scores\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"{self.__class__.__name__}({self.n_rels}, {self.rel_emb.weight.shape})\"\n",
    "\n",
    "\n",
    "\n",
    "class ComplEx(nn.Module):\n",
    "    \"\"\"\n",
    "    ComplEx method but for matrices\n",
    "    \"\"\"\n",
    "    def __init__(self, n_rels, n_features):\n",
    "        super().__init__()\n",
    "        self.n_rels = n_rels\n",
    "        self.n_features = n_features\n",
    "    \n",
    "        # Relation embeddings are also complex\n",
    "        self.rel_real = nn.Embedding(self.n_rels, (self.n_features // 2) * (self.n_features // 2))\n",
    "        self.rel_imag = nn.Embedding(self.n_rels, (self.n_features // 2) * (self.n_features // 2))\n",
    "        \n",
    "        # Initialize embeddings\n",
    "        nn.init.xavier_uniform_(self.rel_real.weight)\n",
    "        nn.init.xavier_uniform_(self.rel_imag.weight)\n",
    "\n",
    "    def forward(self, heads, tails, rels, alpha_scores=None):\n",
    "        # Preprocess\n",
    "        # heads = F.normalize(heads, dim=-1)\n",
    "        # tails = F.normalize(tails, dim=-1)\n",
    "        \n",
    "        r_real, r_imag = self.rel_real(rels), self.rel_imag(rels)\n",
    "        r_real = F.normalize(r_real, dim=-1)\n",
    "        r_imag = F.normalize(r_imag, dim=-1)\n",
    "        # print(r_real.shape)\n",
    "        r_real = r_real.view(-1, self.n_features // 2, self.n_features // 2)\n",
    "        r_imag = r_imag.view(-1, self.n_features // 2, self.n_features // 2)\n",
    "        # print(r_real.shape)\n",
    "        # Split heads and tails to imaginary parts\n",
    "        h_real, h_imag = heads[..., :self.n_features // 2], heads[..., self.n_features // 2:]\n",
    "        t_real, t_imag = tails[..., :self.n_features // 2], heads[..., self.n_features // 2:]\n",
    "\n",
    "        h_real, h_imag = F.normalize(h_real, dim=-1), F.normalize(h_imag, dim=-1)\n",
    "        t_real, t_imag = F.normalize(t_real, dim=-1), F.normalize(t_imag, dim=-1)\n",
    "\n",
    "        # ComplEx scoring functionn\n",
    "        first_part_score = h_real @ r_real @ t_real.transpose(-2, -1)\n",
    "        second_part_score = h_real @ r_imag @ t_imag.transpose(-2, -1)\n",
    "        third_part_score = h_imag @ r_real @ t_imag.transpose(-2, -1)\n",
    "        fourth_part_score = h_imag @ r_imag @ t_real.transpose(-2, -1)\n",
    "\n",
    "        scores = first_part_score + second_part_score + third_part_score + fourth_part_score\n",
    "        \n",
    "        # If alpha_scores is provided, apply it\n",
    "        if alpha_scores is not None:\n",
    "            scores = alpha_scores * scores\n",
    "\n",
    "        scores = scores.sum(dim=(-2, -1))\n",
    "        \n",
    "        return scores\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"{self.__class__.__name__}({self.n_rels}, {self.rel_real.weight.shape}, {self.rel_imag.weight.shape})\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "CWV8Vy2YRXQA"
   },
   "outputs": [],
   "source": [
    "class SSI_DDI(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_features,\n",
    "        hidd_dim,\n",
    "        kge_dim,\n",
    "        rel_total,\n",
    "        heads_out_feat_params,\n",
    "        blocks_params,\n",
    "        sp_ratio,\n",
    "        use_activation_fn,\n",
    "        use_ComplEx,\n",
    "        sp_min_score,\n",
    "        co_attention_method,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        blocks_params: list of number layers for multi-head attentions\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        # not using this one\n",
    "        self.hidd_dim = hidd_dim\n",
    "        self.rel_total = rel_total\n",
    "        self.kge_dim = kge_dim\n",
    "        self.n_blocks = len(blocks_params)\n",
    "\n",
    "        self.initial_norm = LayerNorm(self.in_features)\n",
    "        self.blocks = []\n",
    "        self.use_activation_fn = use_activation_fn\n",
    "        self.use_ComplEx = use_ComplEx\n",
    "        # Layer normalization list\n",
    "        self.net_norms = ModuleList()\n",
    "        for i, (head_out_feats, n_heads) in enumerate(\n",
    "            zip(heads_out_feat_params, blocks_params)\n",
    "        ):\n",
    "            block = SSI_DDI_Block(\n",
    "                n_heads, in_features, head_out_feats, final_out_feats=self.hidd_dim, sp_ratio=sp_ratio, sp_min_score=sp_min_score\n",
    "            )\n",
    "            self.add_module(f\"block{i}\", block)\n",
    "            self.blocks.append(block)\n",
    "            self.net_norms.append(LayerNorm(head_out_feats * n_heads))\n",
    "            in_features = head_out_feats * n_heads\n",
    "\n",
    "        if co_attention_method == \"multihead\":\n",
    "            self.co_attention = MultiheadCoAttentionLayer(self.kge_dim, self.use_activation_fn)\n",
    "        elif co_attention_method == \"improved\":\n",
    "            self.co_attention = CoAttentionLayerImproved(self.kge_dim, self.use_activation_fn)\n",
    "        else:\n",
    "            self.co_attention = CoAttentionLayer(self.kge_dim, self.use_activation_fn)\n",
    "            \n",
    "        if self.use_ComplEx:\n",
    "            self.KGE = ComplEx(self.rel_total, self.kge_dim)\n",
    "        else:\n",
    "            self.KGE = RESCAL(self.rel_total, self.kge_dim)\n",
    "\n",
    "    def forward(self, triples):\n",
    "        h_data, t_data, rels = triples\n",
    "\n",
    "        h_data.x = self.initial_norm(h_data.x, h_data.batch)\n",
    "        t_data.x = self.initial_norm(t_data.x, t_data.batch)\n",
    "\n",
    "        repr_h = []\n",
    "        repr_t = []\n",
    "\n",
    "        for i, block in enumerate(self.blocks):\n",
    "            out1, out2 = block(h_data), block(t_data)\n",
    "\n",
    "            h_data = out1[0]\n",
    "            t_data = out2[0]\n",
    "            r_h = out1[1]\n",
    "            r_t = out2[1]\n",
    "\n",
    "            repr_h.append(r_h)\n",
    "            repr_t.append(r_t)\n",
    "\n",
    "            h_data.x = F.elu(self.net_norms[i](h_data.x, h_data.batch))\n",
    "            t_data.x = F.elu(self.net_norms[i](t_data.x, t_data.batch))\n",
    "\n",
    "        repr_h = torch.stack(repr_h, dim=-2)\n",
    "        repr_t = torch.stack(repr_t, dim=-2)\n",
    "\n",
    "        kge_heads = repr_h\n",
    "        kge_tails = repr_t\n",
    "\n",
    "        attentions = self.co_attention(kge_heads, kge_tails)\n",
    "        # attentions = None\n",
    "        scores = self.KGE(kge_heads, kge_tails, rels, attentions)\n",
    "\n",
    "        return scores\n",
    "\n",
    "\n",
    "class SSI_DDI_Block(nn.Module):\n",
    "    def __init__(self, n_heads, in_features, head_out_feats, final_out_feats, sp_ratio, sp_min_score):\n",
    "        \"\"\"\n",
    "        n_heades: number of multi-head attentions = 2\n",
    "        in_features: number of features = 55 . For explicit valence use, number of features = 56.\n",
    "        head_out_feats: number of out features. For 4 layers: [32, 32, 32, 32]\n",
    "        sp_ratio: SAGPooling ratio\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.n_heads = n_heads\n",
    "        self.in_features = in_features\n",
    "        self.out_features = head_out_feats\n",
    "        self.conv = GATConv(in_features, head_out_feats, n_heads)\n",
    "        # SAGPooling: Ranks nodes based on self-attention scores\n",
    "\n",
    "        if sp_ratio is None and sp_min_score is None:\n",
    "            self.readout = SAGPooling(n_heads * head_out_feats, min_score=-1)\n",
    "        else:\n",
    "            if sp_ratio is not None:\n",
    "                self.readout = SAGPooling(n_heads * head_out_feats, min_score=sp_min_score, ratio=sp_ratio)\n",
    "            else:\n",
    "                self.readout = SAGPooling(n_heads * head_out_feats, min_score=sp_min_score)\n",
    "\n",
    "    def forward(self, data):\n",
    "        data.x = self.conv(data.x, data.edge_index)\n",
    "        # Call SAGPooling here\n",
    "        # If min_score = -1 so nodes will not be filtered out, basically redudant for using the SAGPooling.\n",
    "        att_x, att_edge_index, att_edge_attr, att_batch, att_perm, att_scores = (\n",
    "            self.readout(data.x, data.edge_index, batch=data.batch)\n",
    "        )\n",
    "        # Aggregates the pooled node features (att_x) across the graph to obtain a global representation\n",
    "        global_graph_emb = global_add_pool(att_x, att_batch)\n",
    "\n",
    "        # data = max_pool_neighbor_x(data)\n",
    "        return data, global_graph_emb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "ZT2vnlWzR6FD"
   },
   "outputs": [],
   "source": [
    "class SigmoidLoss(nn.Module):\n",
    "    def __init__(self, adv_temperature=None):\n",
    "        super().__init__()\n",
    "        self.adv_temperature = adv_temperature\n",
    "\n",
    "    def forward(self, p_scores, n_scores):\n",
    "        if self.adv_temperature:\n",
    "            weights = F.softmax(self.adv_temperature * n_scores, dim=-1).detach()\n",
    "            n_scores = weights * n_scores\n",
    "        p_loss = -F.logsigmoid(p_scores).mean()\n",
    "        n_loss = -F.logsigmoid(-n_scores).mean()\n",
    "\n",
    "        return (p_loss + n_loss) / 2, p_loss, n_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section contains compute functions for the results or metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "7jJxwTGhtwRE"
   },
   "outputs": [],
   "source": [
    "def do_compute(model, batch, device, training=True):\n",
    "    \"\"\"\n",
    "    *batch: (pos_tri, neg_tri)\n",
    "    *pos/neg_tri: (batch_h, batch_t, batch_r)\n",
    "    \"\"\"\n",
    "    probas_pred, ground_truth = [], []\n",
    "    pos_tri, neg_tri = batch\n",
    "\n",
    "    pos_tri = [tensor.to(device=device) for tensor in pos_tri]\n",
    "    p_score = model(pos_tri)\n",
    "    probas_pred.append(torch.sigmoid(p_score.detach()).cpu())\n",
    "    ground_truth.append(np.ones(len(p_score)))\n",
    "\n",
    "    neg_tri = [tensor.to(device=device) for tensor in neg_tri]\n",
    "    n_score = model(neg_tri)\n",
    "    probas_pred.append(torch.sigmoid(n_score.detach()).cpu())\n",
    "    ground_truth.append(np.zeros(len(n_score)))\n",
    "\n",
    "    probas_pred = np.concatenate(probas_pred)\n",
    "    ground_truth = np.concatenate(ground_truth)\n",
    "\n",
    "    return p_score, n_score, probas_pred, ground_truth\n",
    "\n",
    "\n",
    "def do_compute_metrics(probas_pred, target):\n",
    "\n",
    "    pred = (probas_pred >= 0.5).astype(np.int64)\n",
    "\n",
    "    acc = metrics.accuracy_score(target, pred)\n",
    "    auc_roc = metrics.roc_auc_score(target, probas_pred)\n",
    "    f1_score = metrics.f1_score(target, pred)\n",
    "\n",
    "    p, r, t = metrics.precision_recall_curve(target, probas_pred)\n",
    "    auc_prc = metrics.auc(r, p)\n",
    "\n",
    "    return acc, auc_roc, auc_prc\n",
    "\n",
    "\n",
    "\n",
    "def export_metrics(train_metrics, val_metrics, epoch):\n",
    "    train_metrics_dir = \"train_metrics\"\n",
    "    metrics_file = f\"{train_metrics_dir}/{model_name}.csv\"\n",
    "    train_loss, train_acc, train_auc_roc, train_auc_prc = train_metrics\n",
    "    val_loss, val_acc, val_auc_roc, val_auc_prc = val_metrics\n",
    "\n",
    "    data = [epoch, train_loss, train_acc, train_auc_roc, train_auc_prc, val_loss, val_acc, val_auc_roc, val_auc_prc]\n",
    "    header = [\"epoch\", \"train_loss\", \"train_acc\", \"train_auc_roc\", \"train_auc_prc\", \"val_loss\", \"val_acc\", \"val_auc_roc\", \"val_auc_prc\"]\n",
    "    \n",
    "    if epoch == 1:\n",
    "        with open(metrics_file, 'w', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            # Write the header\n",
    "            writer.writerow(header)\n",
    "            # Write the data rows\n",
    "            writer.writerow(data)\n",
    "    else:\n",
    "        with open(metrics_file, 'a', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            # Write the data to the file\n",
    "            writer.writerow(data)\n",
    "    \n",
    "\n",
    "def save_model(best, current, met_type):\n",
    "    model_file = model_prc_file\n",
    "    if met_type == \"acc\":\n",
    "        model_file = model_acc_file\n",
    "    elif met_type == \"roc\":\n",
    "        model_file = model_roc_file\n",
    "        \n",
    "    if best < current:\n",
    "        print(f\"Saving model {met_type}\")\n",
    "        best = current\n",
    "        torch.save(model, model_file)\n",
    "    return best\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "EV5GTDPktz_h"
   },
   "outputs": [],
   "source": [
    "def train(\n",
    "    model,\n",
    "    train_data_loader,\n",
    "    val_data_loader,\n",
    "    loss_fn,\n",
    "    optimizer,\n",
    "    n_epochs,\n",
    "    device,\n",
    "    scheduler=None,\n",
    "):\n",
    "    print(\"Starting training at:\", datetime.today())\n",
    "    print(\"Device:\", device)\n",
    "    print_tunning_parameters()\n",
    "    best_val_prc = 0\n",
    "    best_val_acc = 0\n",
    "    best_val_roc = 0\n",
    "    for i in range(1, n_epochs + 1):\n",
    "        start = time.time()\n",
    "        train_loss = 0\n",
    "        train_loss_pos = 0\n",
    "        train_loss_neg = 0\n",
    "        val_loss = 0\n",
    "        val_loss_pos = 0\n",
    "        val_loss_neg = 0\n",
    "        train_probas_pred = []\n",
    "        train_ground_truth = []\n",
    "        val_probas_pred = []\n",
    "        val_ground_truth = []\n",
    "\n",
    "        for batch in train_data_loader:\n",
    "            # print(len(batch))\n",
    "            model.train()\n",
    "            p_score, n_score, probas_pred, ground_truth = do_compute(model, batch, device)\n",
    "            train_probas_pred.append(probas_pred)\n",
    "            train_ground_truth.append(ground_truth)\n",
    "            loss, loss_p, loss_n = loss_fn(p_score, n_score)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item() * len(p_score)\n",
    "        train_loss /= len(train_data)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            train_probas_pred = np.concatenate(train_probas_pred)\n",
    "            train_ground_truth = np.concatenate(train_ground_truth)\n",
    "\n",
    "            train_acc, train_auc_roc, train_auc_prc = do_compute_metrics(\n",
    "                train_probas_pred, train_ground_truth\n",
    "            )\n",
    "\n",
    "            for batch in val_data_loader:\n",
    "                model.eval()\n",
    "                p_score, n_score, probas_pred, ground_truth = do_compute(model, batch, device)\n",
    "                val_probas_pred.append(probas_pred)\n",
    "                val_ground_truth.append(ground_truth)\n",
    "                loss, loss_p, loss_n = loss_fn(p_score, n_score)\n",
    "                val_loss += loss.item() * len(p_score)\n",
    "\n",
    "            val_loss /= len(val_data)\n",
    "            val_probas_pred = np.concatenate(val_probas_pred)\n",
    "            val_ground_truth = np.concatenate(val_ground_truth)\n",
    "            val_acc, val_auc_roc, val_auc_prc = do_compute_metrics(\n",
    "                val_probas_pred, val_ground_truth\n",
    "            )\n",
    "            \n",
    "            # Save model if this is the best so far\n",
    "            best_val_prc = save_model(best_val_prc, val_auc_prc, \"prc\")\n",
    "            best_val_acc = save_model(best_val_acc, val_acc, \"acc\")\n",
    "            best_val_roc = save_model(best_val_roc, val_auc_roc, \"roc\")\n",
    "\n",
    "        if scheduler:\n",
    "            # print('scheduling')\n",
    "            scheduler.step()\n",
    "\n",
    "        # Exporting metrics for later plots\n",
    "        train_metrics = (train_loss, train_acc, train_auc_roc, train_auc_prc)\n",
    "        val_metrics = (val_loss, val_acc, val_auc_roc, val_auc_prc)\n",
    "        export_metrics(train_metrics, val_metrics, i)\n",
    "        \n",
    "        print(\n",
    "            f\"Epoch: {i} ({time.time() - start:.4f}s), train_loss: {train_loss:.4f}, val_loss: {val_loss:.4f},\"\n",
    "            f\" train_acc: {train_acc:.4f}, val_acc:{val_acc:.4f}\"\n",
    "        )\n",
    "        print(\n",
    "            f\"\\t\\ttrain_roc: {train_auc_roc:.4f}, val_roc: {val_auc_roc:.4f}, train_auprc: {train_auc_prc:.4f}, val_auprc: {val_auc_prc:.4f}\"\n",
    "        )\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def predict(model, test_data_loader, device):\n",
    "    print('Starting predicting at', datetime.today())\n",
    "    print('Device', device)\n",
    "\n",
    "    test_probas_pred = []\n",
    "    test_ground_truth = []\n",
    "\n",
    "    # Switch to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():  # No need to calculate gradients during testing\n",
    "        for batch in test_data_loader:\n",
    "            # Get predictions and ground truth for the batch\n",
    "            p_score, n_score, probas_pred, ground_truth = do_compute(model, batch, device, training=False)\n",
    "\n",
    "            # Append the predictions and ground truths\n",
    "            test_probas_pred.append(probas_pred)\n",
    "            test_ground_truth.append(ground_truth)\n",
    "\n",
    "\n",
    "    # Concatenate the results for the entire test dataset\n",
    "    test_probas_pred = np.concatenate(test_probas_pred)\n",
    "    test_ground_truth = np.concatenate(test_ground_truth)\n",
    "\n",
    "    # Calculate the metrics for the test dataset\n",
    "    test_acc, test_auc_roc, test_auc_prc = do_compute_metrics(test_probas_pred, test_ground_truth)\n",
    "\n",
    "    print(f'Test Accuracy: {test_acc:.4f}')\n",
    "    print(f'Test ROC AUC: {test_auc_roc:.4f}')\n",
    "    print(f'Test PRC AUC: {test_auc_prc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 722,
     "status": "ok",
     "timestamp": 1729307125347,
     "user": {
      "displayName": "Nam Le",
      "userId": "10910051602335296858"
     },
     "user_tz": -420
    },
    "id": "PQ1H7udUuFMT",
    "outputId": "4519f4c7-6ad4-4d20-b66d-a5f26cdab691"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SSI_DDI(\n",
      "  (initial_norm): LayerNorm(55, affine=True, mode=graph)\n",
      "  (net_norms): ModuleList(\n",
      "    (0-3): 4 x LayerNorm(64, affine=True, mode=graph)\n",
      "  )\n",
      "  (block0): SSI_DDI_Block(\n",
      "    (conv): GATConv(55, 32, heads=2)\n",
      "    (readout): SAGPooling(GraphConv, 64, min_score=-1, multiplier=1.0)\n",
      "  )\n",
      "  (block1): SSI_DDI_Block(\n",
      "    (conv): GATConv(64, 32, heads=2)\n",
      "    (readout): SAGPooling(GraphConv, 64, min_score=-1, multiplier=1.0)\n",
      "  )\n",
      "  (block2): SSI_DDI_Block(\n",
      "    (conv): GATConv(64, 32, heads=2)\n",
      "    (readout): SAGPooling(GraphConv, 64, min_score=-1, multiplier=1.0)\n",
      "  )\n",
      "  (block3): SSI_DDI_Block(\n",
      "    (conv): GATConv(64, 32, heads=2)\n",
      "    (readout): SAGPooling(GraphConv, 64, min_score=-1, multiplier=1.0)\n",
      "  )\n",
      "  (co_attention): MultiheadCoAttentionLayer(\n",
      "    (W_q): ParameterList(\n",
      "        (0): Parameter containing: [torch.float32 of size 64x32]\n",
      "        (1): Parameter containing: [torch.float32 of size 64x32]\n",
      "    )\n",
      "    (W_k): ParameterList(\n",
      "        (0): Parameter containing: [torch.float32 of size 64x32]\n",
      "        (1): Parameter containing: [torch.float32 of size 64x32]\n",
      "    )\n",
      "    (bias): ParameterList(\n",
      "        (0): Parameter containing: [torch.float32 of size 32]\n",
      "        (1): Parameter containing: [torch.float32 of size 32]\n",
      "    )\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (KGE): ComplEx(86, torch.Size([86, 1024]), torch.Size([86, 1024]))\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "heads_out_feat_params = []\n",
    "block_params = []\n",
    "\n",
    "for _ in range(num_GAT_layers):\n",
    "    heads_out_feat_params.append(kge_dim // 2)\n",
    "    block_params.append(num_GAT_multiheads)\n",
    "\n",
    "if mode == \"train\":\n",
    "    model = SSI_DDI(\n",
    "        n_atom_feats,\n",
    "        n_atom_hid,\n",
    "        kge_dim,\n",
    "        rel_total,\n",
    "        heads_out_feat_params=heads_out_feat_params,\n",
    "        blocks_params=block_params,\n",
    "        sp_ratio=sp_ratio,\n",
    "        use_activation_fn=use_activation_fn,\n",
    "        use_ComplEx=use_ComplEx,\n",
    "        sp_min_score=sp_min_score,\n",
    "        co_attention_method=co_attention_method,\n",
    "    )\n",
    "    loss = SigmoidLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    scheduler = optim.lr_scheduler.LambdaLR(optimizer, lambda epoch: 0.96 ** (epoch))\n",
    "    print(model)\n",
    "    model.to(device=device)\n",
    "\n",
    "    # Train\n",
    "    train(\n",
    "        model,\n",
    "        train_data_loader,\n",
    "        val_data_loader,\n",
    "        loss,\n",
    "        optimizer,\n",
    "        n_epochs,\n",
    "        device,\n",
    "        scheduler,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f04DujiWugRa"
   },
   "outputs": [],
   "source": [
    "# Predict\n",
    "model = torch.load(model_prc_file)\n",
    "print(model)\n",
    "model.to(device=device)\n",
    "predict(model, test_data_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPrjMB1qWDzpoQ4nNpP4AfV",
   "gpuType": "T4",
   "mount_file_id": "1SgLTMIysgeGUK6s_GjIn2Ssygt0A5LZV",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
